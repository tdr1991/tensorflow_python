{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 这里的代码针对的是英伟达（vnidia）的 GPU\n",
    "def check_gpus():\n",
    "    \"\"\"检查是否存在 GPU\"\"\"\n",
    "    if not \"NVIDIA System Management\" in os.popen(\"nvidia-smi -h\").read():\n",
    "        print(\"cuda 工具没有安装\")\n",
    "        return False\n",
    "    gpus_index = os.popen(\"nvidia-smi --query-gpu=index --format=csv,noheader\").readlines()\n",
    "    if len(gpus_index) < 1:\n",
    "        print(\"没有 GPU 存在\")\n",
    "        return False\n",
    "    print(\"存在 GPU，总共有 %d 块 GPU 卡\" % (len(gpus_index)))\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "存在 GPU，总共有 4 块 GPU 卡\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse(line, qargs):\n",
    "    \"\"\"\n",
    "    line：一行文本\n",
    "    qargs：查询参数\n",
    "    解析一行 nvidia-smi 返回 csv 格式文本\n",
    "    \"\"\"\n",
    "    numberic_args = ['memory.free', 'memory.total', 'power.draw', 'power.limit'] #可计数的参数\n",
    "    power_manage_enable=lambda v:(not 'Not Support' in v) #lambda表达式，显卡是否滋瓷power management（笔记本可能不滋瓷）\n",
    "    to_numberic = lambda v:float(v.upper().strip().replace('MIB','').replace('W','')) #带单位字符串去掉单位\n",
    "    process = lambda k,v:((int(to_numberic(v)) if power_manage_enable(v) else 1) if k in numberic_args else v.strip())\n",
    "    return {k:process(k,v) for k,v in zip(qargs,line.strip().split(','))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def query_gpu(qargs=[]):\n",
    "    \"\"\"\n",
    "    qargs: 查询参数\n",
    "    return: a list of dict\n",
    "    Querying GPUs infos\n",
    "    查询GPU信息\n",
    "    \"\"\"\n",
    "    qargs =['index','gpu_name', 'memory.free', 'memory.total', 'power.draw', 'power.limit']+ qargs\n",
    "    cmd = 'nvidia-smi --query-gpu={} --format=csv,noheader'.format(','.join(qargs))\n",
    "    results = os.popen(cmd).readlines()\n",
    "    return [parse(line,qargs) for line in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'gpu_name': 'Tesla K20c',\n",
       "  'index': '0',\n",
       "  'memory.free': 4742,\n",
       "  'memory.total': 4742,\n",
       "  'power.draw': 51,\n",
       "  'power.limit': 225},\n",
       " {'gpu_name': 'Tesla K20c',\n",
       "  'index': '1',\n",
       "  'memory.free': 4742,\n",
       "  'memory.total': 4742,\n",
       "  'power.draw': 49,\n",
       "  'power.limit': 225},\n",
       " {'gpu_name': 'Tesla K20c',\n",
       "  'index': '2',\n",
       "  'memory.free': 4742,\n",
       "  'memory.total': 4742,\n",
       "  'power.draw': 50,\n",
       "  'power.limit': 225},\n",
       " {'gpu_name': 'Tesla K20c',\n",
       "  'index': '3',\n",
       "  'memory.free': 4742,\n",
       "  'memory.total': 4742,\n",
       "  'power.draw': 51,\n",
       "  'power.limit': 225}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def by_power(d):\n",
    "    '''\n",
    "    helper function fo sorting gpus by power\n",
    "    '''\n",
    "    power_infos = (d['power.draw'],d['power.limit'])\n",
    "    if any(v == 1 for v in power_infos):\n",
    "        print('Power management unable for GPU {}'.format(d['index']))\n",
    "        return 1\n",
    "    return float(d['power.draw'])/d['power.limit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sorted_by_power(gpus):\n",
    "    return sorted(gpus, key = by_power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'gpu_name': 'Tesla K20c',\n",
       "  'index': '1',\n",
       "  'memory.free': 4742,\n",
       "  'memory.total': 4742,\n",
       "  'power.draw': 49,\n",
       "  'power.limit': 225},\n",
       " {'gpu_name': 'Tesla K20c',\n",
       "  'index': '2',\n",
       "  'memory.free': 4742,\n",
       "  'memory.total': 4742,\n",
       "  'power.draw': 50,\n",
       "  'power.limit': 225},\n",
       " {'gpu_name': 'Tesla K20c',\n",
       "  'index': '3',\n",
       "  'memory.free': 4742,\n",
       "  'memory.total': 4742,\n",
       "  'power.draw': 50,\n",
       "  'power.limit': 225},\n",
       " {'gpu_name': 'Tesla K20c',\n",
       "  'index': '0',\n",
       "  'memory.free': 4742,\n",
       "  'memory.total': 4742,\n",
       "  'power.draw': 51,\n",
       "  'power.limit': 225}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_by_power(query_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sorted_by_memory(gpus, by_size = False):\n",
    "    \"\"\"\n",
    "    将 GPU 按照显存大小或空闲率排序\n",
    "    \"\"\"\n",
    "    if by_size:\n",
    "        print('Sorted by free memory size')\n",
    "        return sorted(gpus, key = lambda d:d['memory.free'], reverse=True)\n",
    "    else:\n",
    "        print('Sorted by free memory rate')\n",
    "        return sorted(gpus, key = lambda d:float(d['memory.free'])/ d['memory.total'], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted by free memory rate\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'gpu_name': 'Tesla K20c',\n",
       "  'index': '0',\n",
       "  'memory.free': 4742,\n",
       "  'memory.total': 4742,\n",
       "  'power.draw': 51,\n",
       "  'power.limit': 225},\n",
       " {'gpu_name': 'Tesla K20c',\n",
       "  'index': '1',\n",
       "  'memory.free': 4742,\n",
       "  'memory.total': 4742,\n",
       "  'power.draw': 49,\n",
       "  'power.limit': 225},\n",
       " {'gpu_name': 'Tesla K20c',\n",
       "  'index': '2',\n",
       "  'memory.free': 4742,\n",
       "  'memory.total': 4742,\n",
       "  'power.draw': 50,\n",
       "  'power.limit': 225},\n",
       " {'gpu_name': 'Tesla K20c',\n",
       "  'index': '3',\n",
       "  'memory.free': 4742,\n",
       "  'memory.total': 4742,\n",
       "  'power.draw': 50,\n",
       "  'power.limit': 225}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_by_memory(query_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sorted_by_custom(gpus, key, reverse=False, qargs=[]):\n",
    "    if isinstance(key, str) and (key in qargs):\n",
    "        return sorted(gpus, key=lambda d:d[key], reverse=reverse)\n",
    "    if isinstance(key, type(lambda a:a)):\n",
    "        return sorted(gpus, key=key, reverse=reverse)\n",
    "    raise ValueError(\"The argument 'key' must be a function or a key in query args,please read the documention of nvidia-smi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The argument 'key' must be a function or a key in query args,please read the documention of nvidia-smi",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-a34af5e913c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msorted_by_custom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"power.draw\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-be6303d7cdb4>\u001b[0m in \u001b[0;36msorted_by_custom\u001b[0;34m(gpus, key, reverse, qargs)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The argument 'key' must be a function or a key in query args,please read the documention of nvidia-smi\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: The argument 'key' must be a function or a key in query args,please read the documention of nvidia-smi"
     ]
    }
   ],
   "source": [
    "sorted_by_custom(query_gpu(), \"power.draw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GPUManager():\n",
    "    '''\n",
    "    qargs:\n",
    "        query arguments\n",
    "    A manager which can list all available GPU devices\n",
    "    and sort them and choice the most free one.Unspecified \n",
    "    ones pref.\n",
    "    GPU设备管理器，考虑列举出所有可用GPU设备，并加以排序，自动选出\n",
    "    最空闲的设备。在一个GPUManager对象内会记录每个GPU是否已被指定，\n",
    "    优先选择未指定的GPU。\n",
    "    '''\n",
    "    def __init__(self,qargs=[]):\n",
    "        '''\n",
    "        '''\n",
    "        self.qargs=qargs\n",
    "        self.gpus=query_gpu(qargs)\n",
    "        for gpu in self.gpus:\n",
    "            gpu['specified']=False\n",
    "        self.gpu_num=len(self.gpus)\n",
    "    def auto_choice(self, mode=0):\n",
    "        '''\n",
    "        mode:\n",
    "            0:(default)sorted by free memory size\n",
    "        return:\n",
    "            a TF device object\n",
    "        Auto choice the freest GPU device,not specified\n",
    "        ones \n",
    "        自动选择最空闲GPU\n",
    "        '''\n",
    "        for old_infos,new_infos in zip(self.gpus,query_gpu(self.qargs)):\n",
    "            old_infos.update(new_infos)\n",
    "        unspecified_gpus=[gpu for gpu in self.gpus if not gpu['specified']] or self.gpus\n",
    "        \n",
    "        if mode == 0:\n",
    "            print('Choosing the GPU device has largest free memory...')\n",
    "            chosen_gpu = sorted_by_memory(unspecified_gpus, True)[0]\n",
    "        elif mode ==1:\n",
    "            print('Choosing the GPU device has highest free memory rate...')\n",
    "            chosen_gpu=self._sort_by_power(unspecified_gpus)[0]\n",
    "        elif mode == 2:\n",
    "            print('Choosing the GPU device by power...')\n",
    "            chosen_gpu = sorted_by_power(unspecified_gpus)[0]\n",
    "        else:\n",
    "            print('Given an unaviliable mode,will be chosen by memory')\n",
    "            chosen_gpu = sorted_by_memory(unspecified_gpus)[0]\n",
    "        chosen_gpu['specified'] = True\n",
    "        index = chosen_gpu['index']\n",
    "        print('Using GPU {i}:\\n{info}'.format(i=index,info='\\n'.join([str(k)+':'+str(v) for k,v in chosen_gpu.items()])))\n",
    "        return tf.device('/gpu:{}'.format(index))\n",
    "    def get_gpu_num(self):\n",
    "        return self.gpu_num "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "存在 GPU，总共有 4 块 GPU 卡\n",
      "Choosing the GPU device has largest free memory...\n",
      "Sorted by free memory size\n",
      "Using GPU 0:\n",
      "index:0\n",
      "gpu_name:Tesla K20c\n",
      "memory.free:4742\n",
      "memory.total:4742\n",
      "power.draw:51\n",
      "power.limit:225\n",
      "specified:True\n",
      "<contextlib._GeneratorContextManager object at 0x7f60949c0588>\n",
      "\n",
      "\n",
      "Choosing the GPU device has largest free memory...\n",
      "Sorted by free memory size\n",
      "Using GPU 1:\n",
      "index:1\n",
      "gpu_name:Tesla K20c\n",
      "memory.free:4742\n",
      "memory.total:4742\n",
      "power.draw:49\n",
      "power.limit:225\n",
      "specified:True\n",
      "<contextlib._GeneratorContextManager object at 0x7f60949c0710>\n",
      "\n",
      "\n",
      "Choosing the GPU device by power...\n",
      "Using GPU 2:\n",
      "index:2\n",
      "gpu_name:Tesla K20c\n",
      "memory.free:4742\n",
      "memory.total:4742\n",
      "power.draw:50\n",
      "power.limit:225\n",
      "specified:True\n",
      "<contextlib._GeneratorContextManager object at 0x7f60949c05f8>\n"
     ]
    }
   ],
   "source": [
    "if check_gpus():\n",
    "    gm = GPUManager()\n",
    "    print(gm.auto_choice())\n",
    "    print(\"\\n\")\n",
    "    print(gm.auto_choice())\n",
    "    print(\"\\n\")\n",
    "    print(gm.auto_choice(2))\n",
    "else:\n",
    "    print(\"不存在GPU，将使用cpu\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
