{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多进程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 设置单一变量并初始化为 0\n",
    "var = tf.Variable(initial_value=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 每个进程启动一个会话并初始化全部变量\n",
    "sess1 = tf.Session()\n",
    "sess2 = tf.Session()\n",
    "\n",
    "sess1.run(tf.global_variables_initializer())\n",
    "sess2.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial value of var in session 1: 0.0\n",
      "initial value of var in session 2: 0.0\n",
      "increamented var in session 1\n",
      "value of var in session 1: 1.0\n",
      "value of var in session 2: 0.0\n"
     ]
    }
   ],
   "source": [
    "# 每个会话创建单独的执行引擎，各自的变量也互不影响\n",
    "print(\"initial value of var in session 1:\", sess1.run(var))\n",
    "print(\"initial value of var in session 2:\", sess2.run(var))\n",
    "\n",
    "sess1.run(var.assign_add(1.0))\n",
    "print(\"increamented var in session 1\")\n",
    "\n",
    "print(\"value of var in session 1:\", sess1.run(var))\n",
    "print(\"value of var in session 2:\", sess2.run(var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分布式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'hello, distibuted tensorflow!'\n"
     ]
    }
   ],
   "source": [
    "# 对于tensorflow分布式集群，以下代码解释了它的基本原理\n",
    "c = tf.constant(\"hello, distibuted tensorflow!\")\n",
    "# 创建一个本地tensorflow集群\n",
    "server = tf.train.Server.create_local_server()\n",
    "# 在集群上创建一个会话\n",
    "sess = tf.Session(server.target)\n",
    "print(sess.run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在以上代码中，我们先通过 tf.train.Server.create_local_server 在本地创建一个只有一台机器的 TensorFlow 集群。然后在集群上生成一个会话，通过该对话，我们可以将创建的计算图运行在 TensorFlow 集群上。虽然这只是一个单机集群，但它基本上反映了 TensorFlow 集群的工作流程。\n",
    "\n",
    "TensorFlow 集群会通过一系列任务（task）来执行计算图中的运算，一般来说不同的任务会在不同的机器上运行。TensorFlow 集群中的任务也会被聚集为工作（job）。例如在训练深度模型时，一台运行反向传播的机器是一个任务，而所有运行反向传播的集合是一个工作。上面简单的案例只是一个任务的集群，若一个 TensorFlow 集群有多个任务时，我们需要使用 tf.train.ClusterSpec 来指定每一个任务的机器。\n",
    "\n",
    "使用分布式 TensorFlow 训练深度学习模型一般有两种方式，即 in-graph replication 和 between-graph replication。第一种计算图内的分布式会令所有任务都使用一个 TensorFlow 计算图中的变量，而只是将计算部分分配到不同的服务器上。而另一种计算图间的分布式会在每一个计算服务器上创建一个独立的 TensorFlow 计算图，但不同计算图中的相同参数需要以一种固定的方式存放到同一个参数服务器中。以上大概就是分布式 TensorFlow 的基本概念，随后我们将通过具体的案例与代码加深这一部分的理解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了在进程之间共享变量，我们需要将不同的执行引擎连接在一起，并输入分布式张量流。\n",
    "\n",
    "若使用分布式 TensorFlow，每个进程会运行一个特殊的执行引擎：一个 TensorFlow 服务器。服务器作为集群的一部分链接在一起。（群集中的每个服务器也称为任务。）\n",
    "\n",
    "第一步是定义集群的规模。我们从最简单的集群开始：即两台服务器（两个任务），它们都在同一台机器上，一个在 2222 端口，一个在 2223 端口。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tasks = [\"localhost:2222\", \"localhost:2223\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 每一个任务都与工作相关联\n",
    "jobs = {\"local\": tasks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 将所有任务定义为一个集群\n",
    "cluster = tf.train.ClusterSpec(jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们现在可以启动服务器，指定每个服务器对应为集群定义中的哪个服务器。立即启动各服务器，监听集群设置中指定的端口。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 指定服务器对应集群中的服务器，启动服务器时集群监听指定端口\n",
    "server1 = tf.train.Server(cluster, job_name=\"local\", task_index=0)\n",
    "\n",
    "server2 = tf.train.Server(cluster, job_name=\"local\", task_index=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将服务器连接在同一个集群中，我们现在可以体验到分布式 TensorFlow 的强大功能：任何具有相同名称的变量都将在所有服务器之间共享。\n",
    "\n",
    "最简单的例子是在所有的服务器上运行同一张静态计算图，且每个图只有一个变量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "var = tf.Variable(initial_value=0.0, name=\"var\")\n",
    "sess1 = tf.Session(server1.target)\n",
    "sess2 = tf.Session(server2.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial value of var in session 1: 0.0\n",
      "Initial value of var in session 2: 0.0\n",
      "Incremented var in session 1\n",
      "Value of var in session 1: 1.0\n",
      "Value of var in session 2: 1.0\n"
     ]
    }
   ],
   "source": [
    "sess1.run(tf.global_variables_initializer())\n",
    "#sess2.run(tf.global_variables_initializer()) # 因为只有一个变量，而且该变量是共享的，此处在全局初始化有点多余\n",
    "\n",
    "print(\"Initial value of var in session 1:\", sess1.run(var))\n",
    "print(\"Initial value of var in session 2:\", sess2.run(var))\n",
    "\n",
    "sess1.run(var.assign_add(1.0))\n",
    "print(\"Incremented var in session 1\")\n",
    "\n",
    "print(\"Value of var in session 1:\", sess1.run(var))\n",
    "print(\"Value of var in session 2:\", sess2.run(var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 存放\n",
    "\n",
    "现在我们可能会想：变量究竟存储在哪个服务器上？又是哪个服务器在运行操作？\n",
    "\n",
    "按经验来说，变量和操作都默认存储在集群的第一个任务上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_with_location_trace(sess, op):\n",
    "    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "    run_metadata = tf.RunMetadata()\n",
    "    sess.run(op, options=run_options, run_metadata=run_metadata)\n",
    "    for device in run_metadata.step_stats.dev_stats:\n",
    "        print(device.device)\n",
    "        for node in device.node_stats:\n",
    "            print(\"\\t\", node.node_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/job:local/replica:0/task:0/cpu:0\n",
      "\t _SOURCE\n",
      "/job:local/replica:0/task:0/gpu:0\n",
      "\t _SOURCE\n",
      "\t var\n"
     ]
    }
   ],
   "source": [
    "# 在任务 1 中处理变量 var，所有操作将运行在该任务上\n",
    "run_with_location_trace(sess1, var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/job:local/replica:0/task:0/cpu:0\n",
      "\t _SOURCE\n",
      "/job:local/replica:0/task:0/gpu:0\n",
      "\t _SOURCE\n",
      "\t AssignAdd_1/value\n",
      "\t var\n",
      "\t AssignAdd_1\n"
     ]
    }
   ],
   "source": [
    "run_with_location_trace(sess1, var.assign_add(1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/job:local/replica:0/task:1/cpu:0\n",
      "\t _SOURCE\n",
      "/job:local/replica:0/task:0/gpu:0\n",
      "\t _SOURCE\n",
      "\t var\n"
     ]
    }
   ],
   "source": [
    "# 如果在任务 2 中处理变量 var，图节点仍然运行在任务 1 中\n",
    "run_with_location_trace(sess2, var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 使用 tf.device 将操作绑定到指定任务中\n",
    "with tf.device(\"/job:local/task:0\"):\n",
    "    var1 = tf.Variable(0., name=\"var1\")\n",
    "with tf.device(\"/job:local/task:1\"):\n",
    "    var2 = tf.Variable(1., name=\"var2\")\n",
    "sess1.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/job:local/replica:0/task:0/cpu:0\n",
      "\t _SOURCE\n",
      "/job:local/replica:0/task:0/gpu:0\n",
      "\t _SOURCE\n",
      "\t var1\n"
     ]
    }
   ],
   "source": [
    "run_with_location_trace(sess1, var1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是 var2 运行在第二个任务上。即使我们尝试使用连接到第一个任务的会话来评估它，它仍然在第二个任务上运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/job:local/replica:0/task:0/cpu:0\n",
      "\t _SOURCE\n",
      "/job:local/replica:0/task:1/gpu:0\n",
      "\t _SOURCE\n",
      "\t var2\n"
     ]
    }
   ],
   "source": [
    "run_with_location_trace(sess1, var2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/job:local/replica:0/task:1/cpu:0\n",
      "\t _SOURCE\n",
      "/job:local/replica:0/task:0/gpu:0\n",
      "\t _SOURCE\n",
      "\t var1\n"
     ]
    }
   ],
   "source": [
    "run_with_location_trace(sess2, var1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/job:local/replica:0/task:1/cpu:0\n",
      "\t _SOURCE\n",
      "/job:local/replica:0/task:1/gpu:0\n",
      "\t _SOURCE\n",
      "\t var2\n"
     ]
    }
   ],
   "source": [
    "run_with_location_trace(sess2, var2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算图\n",
    "分布式 TensorFlow 处理图的过程有几点需要注意。\n",
    "\n",
    "### 谁构建了这个图？\n",
    "\n",
    "首先，尽管在整个集群中共享变量值，但图并不会自动共享。\n",
    "\n",
    "我们用两台服务器创建一个新的集群，然后用显式创建的图设置第一台服务器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster = tf.train.ClusterSpec({\"local\": [\"localhost:2224\", \"localhost:2225\"]})\n",
    "server1 = tf.train.Server(cluster, job_name=\"local\", task_index=0)\n",
    "server2 = tf.train.Server(cluster, job_name=\"local\", task_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Operation 'var/initial_value' type=Const>, <tf.Operation 'var' type=VariableV2>, <tf.Operation 'var/Assign' type=Assign>, <tf.Operation 'var/read' type=Identity>]\n"
     ]
    }
   ],
   "source": [
    "graph1 = tf.Graph()\n",
    "with graph1.as_default():\n",
    "    var1 = tf.Variable(0., name=\"var\")\n",
    "sess1 = tf.Session(target=server1.target, graph=graph1)\n",
    "print(graph1.get_operations())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们创建连接到第二台服务器的会话，请注意图不会自动获取镜像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "graph2 = tf.Graph()\n",
    "sess2 = tf.Session(target=server2.target, graph=graph2)\n",
    "print(graph2.get_operations())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要访问共享变量，我们必须手动添加一个同名的变量到第二个图中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph2.as_default():\n",
    "    var2 = tf.Variable(0., name=\"var\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess1.run(var1.assign(1.0))\n",
    "sess2.run(var2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关键是：每个服务器负责创建自己的图。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 所有服务器上的图都必须一样吗？\n",
    "\n",
    "到目前为止，我们所有的例子都是在两台服务器上运行相同的图。这被称为图内复制（in-graph replication）。\n",
    "\n",
    "例如，假设我们有一个包含三台服务器的集群。服务器 1 保存共享参数，而服务器 2 和服务器 3 是工作站节点，每个都有本地变量。在图内复制中，每台服务器的图如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../../img/in_graph_replication.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图内复制的问题在于每个服务器都必须具有整个图的副本，包括可能只与其他服务器相关的子图。这可能会导致图变得非常大。\n",
    "\n",
    "另一种方法是图间复制（between-graph replication）。在这里，每个服务器都运行一个只包含共享参数的图，而且任何变量和操作都与单个服务器相关。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../../img/between_graph_replication.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这种方法缩减了图的大小，因此我们推荐使用图间复制。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实践细节\n",
    "在介绍完整示例之前，有几个实践中遇到的细节问题需要讨论一下。\n",
    "\n",
    "**如果在所有服务器互联之前尝试在集群上运行某些程序，会发生什么？**\n",
    "\n",
    "我们再次创建一个双任务集群。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster = tf.train.ClusterSpec({\"local\": [\"localhost:2226\", \"localhost:2227\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这一次，让我们在隔离进程中启动每个服务器。（这允许我们随时关闭服务器，以便再次启动它们进行后续的实验。除了关闭启动服务器的进程之外，目前没有其它办法关闭服务器。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Process\n",
    "from time import sleep\n",
    "\n",
    "def s1():\n",
    "    server1 = tf.train.Server(cluster,\n",
    "                              job_name=\"local\",\n",
    "                              task_index=0)\n",
    "    sess1 = tf.Session(server1.target)\n",
    "    print(\"server 1: running no-op...\")\n",
    "    sess1.run(tf.no_op())\n",
    "    print(\"server 1: no-op run!\")\n",
    "    server1.join() # Block\n",
    "\n",
    "def s2():\n",
    "    for i in range(3):\n",
    "        print(\"server 2: %d seconds left before connecting...\"\n",
    "              % (3 - i))\n",
    "        sleep(1.0)\n",
    "    server2 = tf.train.Server(cluster,\n",
    "                              job_name=\"local\",\n",
    "                              task_index=1)\n",
    "    print(\"server 2: connected!\")\n",
    "    server2.join() # Block\n",
    "\n",
    "# daemon=True so that these processes will definitely be killed\n",
    "# when the notebook restarts\n",
    "p1 = Process(target=s1, daemon=True)\n",
    "p2 = Process(target=s2, daemon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "服务器 1 即刻加入集群，但服务器 2 在连接之前等待了一会儿。结果如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "server 1: running no-op...\n"
     ]
    }
   ],
   "source": [
    "p1.start()\n",
    "p2.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "可以看出，每个服务器都试图在集群上运行一个操作，直到所有的服务器都加入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p1.terminate()\n",
    "p2.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 当服务器脱离集群会怎样？\n",
    "\n",
    "我们用两台服务器建立一个集群。服务器 1 只是反复尝试和运行位于服务器 1 上的 no-op 操作。服务器 2 将在两秒钟后宕机。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server 2 dieing...\n"
     ]
    }
   ],
   "source": [
    "def s1():\n",
    "    server1 = tf.train.Server(cluster,\n",
    "                              job_name=\"local\",\n",
    "                              task_index=0)\n",
    "    \n",
    "    with tf.device(\"/job:local/task:0\"):\n",
    "        no_op = tf.no_op()\n",
    "        \n",
    "    sess1 = tf.Session(server1.target)\n",
    "    for _ in range(6):\n",
    "        print(\"Server 1: about to run no-op...\", end=\"\")\n",
    "        sess1.run(no_op)\n",
    "        print(\"success!\")\n",
    "        sleep(1.0)\n",
    "\n",
    "def s2():\n",
    "    server2 = tf.train.Server(cluster,\n",
    "                              job_name=\"local\",\n",
    "                              task_index=1)\n",
    "    sleep(2.0)\n",
    "    print(\"Server 2 dieing...\")\n",
    "    \n",
    "p1 = Process(target=s1, daemon=True)\n",
    "p2 = Process(target=s2, daemon=True)\n",
    "\n",
    "p1.start()\n",
    "p2.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "短期内，只要我们试图运行的操作不在脱离的服务器上，似乎不会出现问题。（我没有测试过长期运行会发生什么。）\n",
    "\n",
    "如果操作是在脱离的服务器上……"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tdr/miniconda3/envs/py3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/tdr/miniconda3/envs/py3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-31-7403e97fe700>\", line 4, in s1\n",
      "    task_index=0)\n",
      "  File \"/home/tdr/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/training/server_lib.py\", line 145, in __init__\n",
      "    self._server_def.SerializeToString(), status)\n",
      "  File \"/home/tdr/miniconda3/envs/py3/lib/python3.6/contextlib.py\", line 88, in __exit__\n",
      "    next(self.gen)\n",
      "  File \"/home/tdr/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\n",
      "    pywrap_tensorflow.TF_GetCode(status))\n",
      "tensorflow.python.framework.errors_impl.UnknownError: Could not start gRPC server\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server 2 dieing...\n"
     ]
    }
   ],
   "source": [
    "def s1():\n",
    "    server1 = tf.train.Server(cluster,\n",
    "                              job_name=\"local\",\n",
    "                              task_index=0)\n",
    "    \n",
    "    # This time, we place the no-op on server 2,\n",
    "    # which is going to leave\n",
    "    with tf.device(\"/job:local/task:1\"):\n",
    "        no_op = tf.no_op()\n",
    "        \n",
    "    sess1 = tf.Session(server1.target)\n",
    "    for _ in range(5):\n",
    "        print(\"Server 1: about to run no-op...\", end=\"\")\n",
    "        sess1.run(no_op)\n",
    "        print(\"success!\")\n",
    "        sleep(1.0)\n",
    "    \n",
    "p1 = Process(target=s1, daemon=True)\n",
    "p2 = Process(target=s2, daemon=True)\n",
    "\n",
    "p1.start()\n",
    "p2.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p1.terminate()\n",
    "p2.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tdr/miniconda3/envs/py3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/tdr/miniconda3/envs/py3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-31-7403e97fe700>\", line 4, in s1\n",
      "    task_index=0)\n",
      "  File \"/home/tdr/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/training/server_lib.py\", line 145, in __init__\n",
      "    self._server_def.SerializeToString(), status)\n",
      "  File \"/home/tdr/miniconda3/envs/py3/lib/python3.6/contextlib.py\", line 88, in __exit__\n",
      "    next(self.gen)\n",
      "  File \"/home/tdr/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\n",
      "    pywrap_tensorflow.TF_GetCode(status))\n",
      "tensorflow.python.framework.errors_impl.UnknownError: Could not start gRPC server\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server 2 dieing...\n",
      "Restarting server 2...\n",
      "Server 2 dieing...\n"
     ]
    }
   ],
   "source": [
    "p1 = Process(target=s1, daemon=True)\n",
    "p2 = Process(target=s2, daemon=True)\n",
    "p1.start()\n",
    "p2.start()\n",
    "sleep(3.0)\n",
    "# At this point, server 1 is blocked, and server 2 is dead.\n",
    "print(\"Restarting server 2...\")\n",
    "p2 = Process(target=s2, daemon=True)\n",
    "p2.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 谁负责初始化共享变量？\n",
    "\n",
    "一种方法是让所有工作站运行 tf.global_variables_initializer()。\n",
    "\n",
    "但是如果我们想保持代码整洁并且只用一个服务器进行初始化，那么如果有其他服务器在初始化之前尝试使用这些变量，可能会遇到问题。一个解决方案就是让其他工作站等待，直到使用 tf.report_uninitialized_variables 的初始化开始。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-10:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tdr/miniconda3/envs/py3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/tdr/miniconda3/envs/py3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-34-c1c49751100d>\", line 4, in s1\n",
      "    task_index=0)\n",
      "  File \"/home/tdr/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/training/server_lib.py\", line 145, in __init__\n",
      "    self._server_def.SerializeToString(), status)\n",
      "  File \"/home/tdr/miniconda3/envs/py3/lib/python3.6/contextlib.py\", line 88, in __exit__\n",
      "    next(self.gen)\n",
      "  File \"/home/tdr/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\n",
      "    pywrap_tensorflow.TF_GetCode(status))\n",
      "tensorflow.python.framework.errors_impl.UnknownError: Could not start gRPC server\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server 2: waiting 3 seconds before initializing...\n",
      "Server 2: waiting 2 seconds before initializing...\n",
      "Server 2: waiting 1 seconds before initializing...\n"
     ]
    }
   ],
   "source": [
    "def s1():\n",
    "    server1 = tf.train.Server(cluster,\n",
    "                              job_name=\"local\",\n",
    "                              task_index=0)\n",
    "    var = tf.Variable(0.0, name='var')\n",
    "    sess1 = tf.Session(server1.target)\n",
    "    \n",
    "    print(\"Server 1: waiting for connection...\")\n",
    "    sess1.run(tf.report_uninitialized_variables())\n",
    "    while len(sess1.run(tf.report_uninitialized_variables())) > 0:\n",
    "        print(\"Server 1: waiting for initialization...\")\n",
    "        sleep(1.0)\n",
    "    print(\"Server 1: variables initialized!\")\n",
    "\n",
    "def s2():\n",
    "    server2 = tf.train.Server(cluster,\n",
    "                              job_name=\"local\",\n",
    "                              task_index=1)\n",
    "    var = tf.Variable(0.0, name='var')\n",
    "    sess2 = tf.Session(server2.target)\n",
    "    \n",
    "    for i in range(3):\n",
    "        print(\"Server 2: waiting %d seconds before initializing...\"\n",
    "              % (3 - i))\n",
    "        sleep(1.0)\n",
    "    sess2.run(tf.global_variables_initializer())\n",
    "    \n",
    "p1 = Process(target=s1, daemon=True)\n",
    "p2 = Process(target=s2, daemon=True)\n",
    "p1.start()\n",
    "p2.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p1.terminate()\n",
    "p2.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 示例\n",
    "\n",
    "让我们把所学的知识融合到最后一个使用多进程的例子中。\n",
    "\n",
    "我们将创建：\n",
    "\n",
    "一个存储单个变量 var 的参数服务器。\n",
    "两个工作站任务（worker task），每个工作站将多次增加变量 var 的值。\n",
    "\n",
    "我们将让参数服务器多输出几次 var 的值，以便查看其变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from multiprocessing import Process\n",
    "from time import sleep\n",
    "\n",
    "cluster = tf.train.ClusterSpec({\n",
    "    \"worker\": [\n",
    "        \"localhost:3333\",\n",
    "        \"localhost:3334\",\n",
    "    ],\n",
    "    \"ps\": [\n",
    "        \"localhost:3335\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "def parameter_server():\n",
    "    with tf.device(\"/job:ps/task:0\"):\n",
    "        var = tf.Variable(0.0, name='var')\n",
    "\n",
    "    server = tf.train.Server(cluster,\n",
    "                             job_name=\"ps\",\n",
    "                             task_index=0)\n",
    "    sess = tf.Session(target=server.target)\n",
    "    \n",
    "    print(\"Parameter server: waiting for cluster connection...\")\n",
    "    sess.run(tf.report_uninitialized_variables())\n",
    "    print(\"Parameter server: cluster ready!\")\n",
    "    \n",
    "    print(\"Parameter server: initializing variables...\")\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"Parameter server: variables initialized\")\n",
    "    \n",
    "    for i in range(5):\n",
    "        val = sess.run(var)\n",
    "        print(\"Parameter server: var has value %.1f\" % val)\n",
    "        sleep(1.0)\n",
    "\n",
    "    print(\"Parameter server: blocking...\")\n",
    "    server.join()\n",
    "    \n",
    "\n",
    "def worker(worker_n):\n",
    "    with tf.device(\"/job:ps/task:0\"):\n",
    "        var = tf.Variable(0.0, name='var')\n",
    "        \n",
    "    server = tf.train.Server(cluster,\n",
    "                             job_name=\"worker\",\n",
    "                             task_index=worker_n)\n",
    "    sess = tf.Session(target=server.target)\n",
    "    \n",
    "    print(\"Worker %d: waiting for cluster connection...\" % worker_n)\n",
    "    sess.run(tf.report_uninitialized_variables())\n",
    "    print(\"Worker %d: cluster ready!\" % worker_n)\n",
    "    \n",
    "    while sess.run(tf.report_uninitialized_variables()):\n",
    "        print(\"Worker %d: waiting for variable initialization...\" % worker_n)\n",
    "        sleep(1.0)\n",
    "    print(\"Worker %d: variables initialized\" % worker_n)\n",
    "    \n",
    "    for i in range(5):\n",
    "        print(\"Worker %d: incrementing var\" % worker_n)\n",
    "        sess.run(var.assign_add(1.0))\n",
    "        sleep(1.0)\n",
    "    \n",
    "    print(\"Worker %d: blocking...\" % worker_n)\n",
    "    server.join()\n",
    "\n",
    "ps_proc = Process(target=parameter_server, daemon=True)\n",
    "w1_proc = Process(target=worker, args=(0, ), daemon=True)\n",
    "w2_proc = Process(target=worker, args=(1, ), daemon=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter server: waiting for cluster connection...\n"
     ]
    }
   ],
   "source": [
    "ps_proc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 0: waiting for cluster connection...\n"
     ]
    }
   ],
   "source": [
    "w1_proc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "cannot start a process twice",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-325ea8a9dc09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw2_proc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.6/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mStart\u001b[0m \u001b[0mchild\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         '''\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cannot start a process twice'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_pid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                \u001b[0;34m'can only start a process object created by current process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: cannot start a process twice"
     ]
    }
   ],
   "source": [
    "w2_proc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for proc in [w1_proc, w2_proc, ps_proc]:\n",
    "    proc.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 总结\n",
    "\n",
    "通过本文，我们了解了：\n",
    "\n",
    "- 如何将多个 TensorFlow 执行引擎（运行在不同进程或不同机器上）集成为一个集群，以便共享变量。\n",
    "- 如何为变量或操作指定服务器。\n",
    "- 图内复制与图间复制。\n",
    "- 在所有服务器互联之前或在服务器脱离集群之后在集群上运行操作，会发生什么。\n",
    "- 如何等待变量被集群中的另一个任务初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "834px",
    "left": "0px",
    "right": "1563px",
    "top": "106px",
    "width": "357px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
